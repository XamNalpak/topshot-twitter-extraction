{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd07d5ecdfd88d71d9992d04b8d02066e33925b5773a00975a361e7c77682b530b7",
   "display_name": "Python 3.8.5 64-bit ('maxkp': virtualenv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Written by Max Paul\n",
    "# April 18, 2021\n",
    "# Cleaning data to get only the top shot username from the tweets\n",
    "# Then pushing the final table back to the SQL data base\n",
    "# experimenting in automating this entire process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### function for checking for new data to add to the database!\n",
    "## so we dont raise errors -- this is in testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_db_dups(df, tablename, engine, dup_cols=[],\n",
    "                         filter_continuous_col=None, filter_categorical_col=None):\n",
    "    \"\"\"\n",
    "    Remove rows from a dataframe that already exist in a database\n",
    "    Required:\n",
    "        df : dataframe to remove duplicate rows from\n",
    "        engine: SQLAlchemy engine object\n",
    "        tablename: tablename to check duplicates in\n",
    "        dup_cols: list or tuple of column names to check for duplicate row values\n",
    "    Optional:\n",
    "        filter_continuous_col: the name of the continuous data column for BETWEEEN min/max filter\n",
    "                               can be either a datetime, int, or float data type\n",
    "                               useful for restricting the database table size to check\n",
    "        filter_categorical_col : the name of the categorical data column for Where = value check\n",
    "                                 Creates an \"IN ()\" check on the unique values in this column\n",
    "    Returns\n",
    "        Unique list of values from dataframe compared to database table\n",
    "    \"\"\"\n",
    "    args = 'SELECT %s FROM %s' %(', '.join(['\"{0}\"'.format(col) for col in dup_cols]), tablename)\n",
    "    args_contin_filter, args_cat_filter = None, None\n",
    "    if filter_continuous_col is not None:\n",
    "        if df[filter_continuous_col].dtype == 'datetime64[ns]':\n",
    "            args_contin_filter = \"\"\" \"%s\" BETWEEN Convert(datetime, '%s')\n",
    "                                          AND Convert(datetime, '%s')\"\"\" %(filter_continuous_col,\n",
    "                              df[filter_continuous_col].min(), df[filter_continuous_col].max())\n",
    "\n",
    "\n",
    "    if filter_categorical_col is not None:\n",
    "        args_cat_filter = ' \"%s\" in(%s)' %(filter_categorical_col,\n",
    "                          ', '.join([\"'{0}'\".format(value) for value in df[filter_categorical_col].unique()]))\n",
    "\n",
    "    if args_contin_filter and args_cat_filter:\n",
    "        args += ' Where ' + args_contin_filter + ' AND' + args_cat_filter\n",
    "    elif args_contin_filter:\n",
    "        args += ' Where ' + args_contin_filter\n",
    "    elif args_cat_filter:\n",
    "        args += ' Where ' + args_cat_filter\n",
    "\n",
    "    df.drop_duplicates(dup_cols, keep='last', inplace=True)\n",
    "    df = pd.merge(df, pd.read_sql(args, engine), how='left', on=dup_cols, indicator=True)\n",
    "    df = df[df['_merge'] == 'left_only']\n",
    "    df.drop(['_merge'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "source": [
    "### Connecting to my data base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "conn = psycopg2.connect(host=\"localhost\",\n",
    "                        database=\"top-twit-mapping\",\n",
    "                        port=5432,\n",
    "                        user='postgres',\n",
    "                        password=3301)"
   ]
  },
  {
   "source": [
    "### Querying the database to only select the tweets that contain 'TS\"\n",
    "#### Also transforming the data into a dataframe for operations and datacleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# creating an instance of an operation\n",
    "cur = conn.cursor()\n",
    "\n",
    "# our command we want to send to the database\n",
    "command =   '''\n",
    "            select user_id,tweet\n",
    "            from twitter_tweet\n",
    "            where to_tsvector('english',tweet) @@ plainto_tsquery('english','TS')\n",
    "            '''\n",
    "\n",
    "# executing the command\n",
    "cur.execute(command)\n",
    "\n",
    "# extracting all the data\n",
    "tupples = cur.fetchall()\n",
    "# closing this connection\n",
    "cur.close()\n",
    "# col names\n",
    "column_names = ['user_id','tweet']\n",
    "# to a dataframe\n",
    "data = pd.DataFrame(tupples, columns=column_names)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "source": [
    "# Preprocessing the data for username extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-88-8955b68ed43f>:5: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe[tweet] = dataframe[tweet].str.replace(r'http\\S+', '')\n<ipython-input-88-8955b68ed43f>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe[tweet] = dataframe[tweet].str.replace(r'@\\w+', '')\n<ipython-input-88-8955b68ed43f>:9: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe[tweet] = dataframe[tweet].str.replace(r'\\s+', '')\n<ipython-input-88-8955b68ed43f>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n  dataframe[tweet] = dataframe[tweet].str.replace(r'RT\\S+', '')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess(dataframe,tweet):\n",
    "\n",
    "    # remove links\n",
    "    dataframe[tweet] = dataframe[tweet].str.replace(r'http\\S+', '')\n",
    "    # remove mentions\n",
    "    dataframe[tweet] = dataframe[tweet].str.replace(r'@\\w+', '')\n",
    "    # remove multiple spaces\n",
    "    dataframe[tweet] = dataframe[tweet].str.replace(r'\\s+', '')\n",
    "    # remove retweets\n",
    "    dataframe[tweet] = dataframe[tweet].str.replace(r'RT\\S+', '')\n",
    "    return dataframe[tweet]\n",
    "\n",
    "\n",
    "data['tweet'] = preprocess(data,'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['tweet'] != '']"
   ]
  },
  {
   "source": [
    "#### TS NAME extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].str.split(':|-.').str[1]"
   ]
  },
  {
   "source": [
    "# FINAL USER ID ALONG WITH THEIR TOPSHOT USER NAME FROM THE TWEET LETS GOOOOO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PUSHING TO THE DATABASE\n",
    "\n",
    "- notes, cant just call the whole column to insert you have to do it row by row"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host=\"localhost\",\n",
    "                        database=\"top-twit-mapping\",\n",
    "                        port=5432,\n",
    "                        user='postgres',\n",
    "                        password=3301)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['user_id','ts_username']\n",
    "\n",
    "\n",
    "data.to_csv('data.csv',index=False)\n",
    "\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset='user_id',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING FOR DUPLICATES !!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3 added to the topshot_username table!\n\n\n\n##################################################\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://postgres:3301@localhost:5432/top-twit-mapping')\n",
    "\n",
    "data = clean_df_db_dups(data, 'topshot_username', engine, dup_cols=['user_id'],filter_continuous_col=None, filter_categorical_col=None)\n",
    "data.head(10)\n",
    "if len(data) >= 1:\n",
    "    engine = create_engine('postgresql://postgres:3301@localhost:5432/top-twit-mapping')\n",
    "    data.to_sql('topshot_username', engine,if_exists='append',index=False)\n",
    "    print(f'{len(data)} added to the topshot_username table!\\n\\n\\n')\n",
    "    print('#'*50)\n",
    "    data\n",
    "else:\n",
    "    print('Nothing to add try again later.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}